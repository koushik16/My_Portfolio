<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Research | Koushik Reddy Parukola</title>
  <link rel="stylesheet" href="stylesheets/styles.css">
</head>
<body>
  <div class="wrapper">
    <header>
      <h1>Koushik Reddy Parukola</h1>
      <p>This page is part of <a href="index.html">Koushik Reddy Parukola's portfolio</a></p>
      <p><a href="index.html">&larr; Back to Home</a></p>
    </header>

<section class="section" id="quantum-encoding">
  <h2>Quantum Encoding</h2>
  <p>Quantum encoding involves representing classical data in quantum states, enabling quantum computers to process information efficiently. This research focuses on developing encoding schemes that optimize data representation for quantum algorithms, enhancing computational speed and accuracy.</p>
  <ul>
    <li>Exploring various quantum data embedding techniques to improve information processing.</li>
    <li>Investigating the impact of encoding methods on quantum machine learning models.</li>
  </ul>
  <p>For an in-depth review, refer to the paper: <a href="https://ieeexplore.ieee.org/document/10479501" target="_blank">Beyond Bits: A Review of Quantum Embedding Techniques for Efficient Information Processing</a>.</p>
</section>


    <!-- Quantum NLP Section -->
    <section class="section" id="quantum-nlp">
      <h2>Quantum NLP - Similarity Comparison and Semantic Properties</h2>
      <p>This area explores the application of quantum computing to Natural Language Processing (NLP), particularly in measuring semantic similarity and analyzing linguistic properties. Quantum algorithms offer potential advantages in processing complex language structures and meanings.</p>
      <ul>
        <li>Developing quantum algorithms for semantic similarity assessments.</li>
        <li>Analyzing the semantic properties of language using quantum computational models.</li>
      </ul>
      <p>For a comprehensive survey, see: <a href="https://aclanthology.org/2021.emnlp-main.254.pdf" target="_blank">Natural Language Processing Meets Quantum Physics: A Survey and Outlook</a>.</p>
    </section>
    

    <!-- Quantum ML Section -->
    <section class="section" id="quantum-ml">
      <h2>Quantum Machine Learning (QML)</h2>
      <p>Quantum Machine Learning integrates quantum computing with classical machine learning techniques to enhance computational capabilities. This research includes developing Quantum Neural Networks (QNNs), implementing Quantum Fourier Transform, exploring Shor's Algorithm for factorization, and optimizing Hamiltonians for problem-solving.</p>
      <ul>
        <li>Designing QNNs for complex data classification and regression tasks.</li>
        <li>Applying Quantum Fourier Transform in signal processing and data analysis.</li>
        <li>Utilizing Shor's Algorithm for efficient integer factorization.</li>
        <li>Optimizing Hamiltonians to solve complex optimization problems.</li>
      </ul>
      <p>For insights into quantum data encoding in machine learning, refer to: <a href="https://arxiv.org/abs/2410.09121" target="_blank">Comparing Quantum Encoding Techniques</a>.</p>
    </section>
    

    <!-- LLM Analysis for NER Triples Section -->
    <section class="section" id="llm-analysis">
      <h2>LLM Analysis for NER Triples in Knowledge Graphs</h2>
      <p>This research focuses on extracting Named Entity Recognition (NER) triples from Large Language Models (LLMs) to construct and enhance knowledge graphs. By identifying entities and their relationships, we aim to improve information retrieval and semantic understanding in various applications.</p>
      <ul>
        <li>Developing methods to extract NER triples from LLM outputs.</li>
        <li>Integrating extracted triples into knowledge graphs for enhanced data representation.</li>
      </ul>
      <p>For methodologies on NER and knowledge graph construction, see: <a href="https://www.mdpi.com/2076-3417/12/11/5651" target="_blank">Quantum Natural Language Processing: Challenges and Opportunities</a>.</p>
    </section>
    

    <!-- Word Embedding Vectors Section -->
    <section class="section" id="word-embedding-vectors">
      <h2>Word Embedding Vectors - Clustering for Semantic and Syntactic Relations</h2>
      <p>This study involves clustering word embedding vectors to uncover semantic and syntactic relationships between words. By grouping similar word vectors, we aim to enhance the understanding of language structures and improve NLP models.</p>
      <ul>
        <li>Applying clustering algorithms to word embeddings to identify semantic groupings.</li>
        <li>Analyzing syntactic patterns through vector space representations.</li>
      </ul>
      <p>For a detailed analysis, refer to: <a href="https://link.springer.com/article/10.1007/s13218-024-00861-w" target="_blank">Quantum Natural Language Processing</a>.</p>
    </section>
    
    <!-- Word Embeddings Dimension Reduction Section -->
    <section class="section" id="word-embedding-dim-reduction">
      <h2>Word Embeddings Dimension Reduction</h2>
      <p>This research focuses on reducing the dimensionality of word embeddings to create more efficient data representations while preserving essential semantic and syntactic information. Techniques such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are employed to achieve this.</p>
      <ul>
        <li>Implementing PCA and t-SNE for dimensionality reduction in word embeddings.</li>
        <li>Evaluating the impact of reduced dimensions on embedding integrity and performance.</li>
      </ul>
    </section>
      </div>
</body>
</html>
